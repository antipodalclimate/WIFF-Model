{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "from tensorflow.keras.models import load_model\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = '/Users/chorvat/Dropbox (Brown)/Research Projects/Active/WIFF-Model/'\n",
    "misc_path = repo_path + 'Misc/NN_params.mat'\n",
    "class_string = repo_path + 'Classifier/Classifier_v1.h5'\n",
    "out_string = repo_path + 'Classifier/Classifier_v1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=None):\n",
    "    x = x - x.max(axis=axis, keepdims=True)\n",
    "    y = np.exp(x)\n",
    "    return y / y.sum(axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0.0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_class(x):\n",
    "    \n",
    "    # x is 27xN - full spectrum (25) + ice thickness (1) + ice concentration (1)\n",
    "    # which gets passed from input to the second layer\n",
    "    # class_weights[0] is input weights. class_weights[1] is the bias. Relu is the activation fcn\n",
    "    y1 = relu(np.dot(x,class_weights[0]) + class_weights[1])\n",
    "    print(np.shape(y1))\n",
    "    # Now what gets passed from input layer 1 to 2\n",
    "    # class_weights[2] is 1-2 weights. class_weights[3] is bias\n",
    "    y2 = relu(np.dot(y1,class_weights[2]) + class_weights[3])\n",
    "    \n",
    "    # Now to output layer \n",
    "    # softmax function should return two numbers that add to 1. \n",
    "    y3 = softmax(np.dot(y2,class_weights[4]) + class_weights[5])\n",
    "\n",
    "    return y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Layer 1 has 2700 elements and has a shape: (27, 100)\n",
      "Weight Layer 2 has 100 elements and has a shape: (100,)\n",
      "Weight Layer 3 has 10000 elements and has a shape: (100, 100)\n",
      "Weight Layer 4 has 100 elements and has a shape: (100,)\n",
      "Weight Layer 5 has 200 elements and has a shape: (100, 2)\n",
      "Weight Layer 6 has 2 elements and has a shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "# SAVING CLASSIFIER\n",
    "NN_class = load_model(class_string);\n",
    "class_weights = NN_class.get_weights()\n",
    "\n",
    "mylist = []\n",
    "i = 0; \n",
    "\n",
    "for arr in class_weights:\n",
    "    i = i + 1\n",
    "    \n",
    "    sublist = list(arr.flatten())\n",
    "    \n",
    "    print('Weight Layer ' + str(i) + ' has ' + str(len(sublist)) + ' elements and has a shape: ' + str(np.shape(arr)))\n",
    "\n",
    "    for a in sublist:\n",
    "        mylist.append(a)\n",
    "        \n",
    "file1 = open(out_string,\"w\")\n",
    "file1.write(str(mylist)[1:-1])\n",
    "file1.close() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      " NN predicts class values of [[0.0310641 0.9689359]]\n",
      " Python NN code predicts class values of [[0.03106418 0.96893582]]\n",
      "[[-7.78154305e-08  8.89913013e-08]]\n"
     ]
    }
   ],
   "source": [
    "# Example of how to execute this code. \n",
    "x = np.expand_dims(np.random.rand(27),axis=1).T;\n",
    "\n",
    "# Sanity check - these are the same\n",
    "class_net = NN_class.predict(x);\n",
    "classout = nn_class(x);\n",
    "\n",
    "print(' NN predicts class values of ' + str(class_net))\n",
    "print(' Python NN code predicts class values of ' + str(classout))\n",
    "\n",
    "print(class_net - classout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

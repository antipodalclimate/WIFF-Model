{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "locstr = '/Users/chorvat/Dropbox (Brown)/Research Projects/Active/Neural-Net-Waves/Use_Conc/'\n",
    "\n",
    "architecture = [(100,100)]\n",
    "\n",
    "repo_string = '/Users/chorvat/Dropbox (Brown)/Research Projects/Active/WIFF-Model/'\n",
    "training_data_string = '/Users/chorvat/Dropbox (Brown)/Research Projects/Active/Data/Neural_Net_Data/6-hourly-2009/Training_converged.mat'\n",
    "network_save_string = '/Users/chorvat/Dropbox (Brown)/Research Projects/Active/Data/Neural_Net_Data/Networks/Classifier/'\n",
    "\n",
    "misc_string = repo_string + 'Misc/NN_params.mat'\n",
    "# These are for where to save \"pickles\" of the training/validation datasets. Should only create one on which any potential architectures or loss functions are trained against\n",
    "pickle_string_class = '/Users/chorvat/Dropbox (Brown)/Research Projects/Active/Data/Neural_Net_Data/Training_Pickles/Use_Conc/Classifier/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Here is imported as a matlab file. \n",
    "training_data = h5py.File(training_data_string, 'r')\n",
    "misc_data = loadmat(misc_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vec = training_data['in'][()].T\n",
    "output_vec = training_data['out'][()].T\n",
    "\n",
    "Freq = misc_data['f']\n",
    "dFreq = Freq * (np.sqrt(1.1) - np.sqrt(1/1.1)); \n",
    "Redge = misc_data['redge'].T\n",
    "Rcent = misc_data['rcent'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5087436, 2660124 should run SP-WIFF\n"
     ]
    }
   ],
   "source": [
    "ice_conc = np.expand_dims(input_vec[:,26],axis=1)\n",
    "ice_thick = np.expand_dims(input_vec[:,25],axis=1)\n",
    "wave_spec = input_vec[:,:25]\n",
    "\n",
    "# wave_spec[wave_spec < 10**-8] = 0\n",
    "wave_energy = np.sum(wave_spec*np.expand_dims(dFreq,axis=1).T,axis=1)\n",
    "peak_loc = np.argmax(wave_spec,axis=1)\n",
    "peak_freq = Freq[peak_loc]\n",
    "\n",
    "num_frac = np.sum(output_vec,axis=1)\n",
    "usable = num_frac > 0\n",
    "print('Out of ' + str(len(num_frac)) + ', ' + str(sum(usable)) + ' should run SP-WIFF')\n",
    "\n",
    "classifier_input = np.concatenate((wave_spec,ice_thick,ice_conc),axis=1) # input_vec[:,:29] # ,peak_freq]).T\n",
    "classifier_output = to_categorical(usable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found previously segmented data\n"
     ]
    }
   ],
   "source": [
    "# Make sure we keep the training and test data the same at all times\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "\n",
    "    X_train = pickle.load( open( pickle_string_class + \"X_train.p\", \"rb\" ) )\n",
    "    X_test = pickle.load( open( pickle_string_class + \"X_test.p\", \"rb\" ) )\n",
    "    y_train = pickle.load( open(pickle_string_class + \"y_train.p\",\"rb\"))\n",
    "    y_test = pickle.load( open(pickle_string_class + \"y_test.p\",\"rb\"))\n",
    "    print('found previously segmented data')\n",
    "\n",
    "except:\n",
    "        \n",
    "        print('exception in loading segmented data')\n",
    "        print('Creating new pickle files')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(classifier_input,classifier_output,test_size=0.3,stratify=classifier_output)\n",
    "        \n",
    "        pickle.dump(X_train, open(pickle_string_class + \"X_train.p\", \"wb\" ) )\n",
    "        pickle.dump(X_test, open(pickle_string_class + \"X_test.p\", \"wb\" ) )\n",
    "        pickle.dump(y_train, open(pickle_string_class + \"y_train.p\", \"wb\" ) )\n",
    "        pickle.dump(y_test, open(pickle_string_class + \"y_test.p\", \"wb\" ) )\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for structno in range(len(architecture)):\n",
    "\n",
    "    struct = architecture[structno]\n",
    "    \n",
    "    print('Size is ' + str(struct))\n",
    "\n",
    "    classifier = Sequential();\n",
    "    #First Hidden Layer - 27 inputs are spectrum + thickness + concentration\n",
    "    classifier.add(Dense(struct[0], activation='relu', kernel_initializer='GlorotNormal', input_dim=27));\n",
    "    print('Adding layer with size ' + str(struct[0]))\n",
    "\n",
    "    #Other Hidden Layers\n",
    "    for i in range(len(struct)-1):\n",
    "        print('Adding layer with size ' + str(struct[i+1]))\n",
    "        classifier.add(Dense(struct[i+1], activation='relu', kernel_initializer='GlorotNormal'));\n",
    "\n",
    "    classifier.add(Dense(2, activation='softmax', kernel_initializer='GlorotNormal'));\n",
    "\n",
    "    #Output Layer\n",
    "    classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy']);\n",
    "\n",
    "    # Output name\n",
    "    outstr = 'Class-'\n",
    "\n",
    "    for i in range(len(struct)):\n",
    "        outstr = outstr + str(struct[i]) + 'x'\n",
    "\n",
    "    outstr = outstr[:-1]\n",
    "\n",
    "    fname = network_save_string + outstr + '.h5'\n",
    "\n",
    "    print(fname)\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n",
    "    \n",
    "    classifier.fit(X_train,y_train,validation_data = (X_test,y_test),batch_size=2048, epochs=1,callbacks = [es])\n",
    "    \n",
    "    classifier.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_string = repo_string + 'Classifier/Classifier_v1.h5'  \n",
    "classifier = load_model(net_string)\n",
    "thresh = 0.54\n",
    "\n",
    "y_pred_class =classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[651360  76834]\n",
      " [111686 686351]]\n"
     ]
    }
   ],
   "source": [
    "y_true = y_test[:,1]\n",
    "y_pred = y_pred_class[:,1] > thresh\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "fnr = fn / [fn + tp]\n",
    "fpr = fp / [fp + tn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive rate is 10.55%\n",
      "false negative rate is 14.0%\n"
     ]
    }
   ],
   "source": [
    "print('false positive rate is ' + str(round(100*fpr[0],2)) + '%')\n",
    "print('false negative rate is ' + str(round(100*fnr[0],2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find results for each threshold\n",
    "\n",
    "nthresh = 500\n",
    "\n",
    "thresher = np.linspace(0,1,nthresh)\n",
    "fnr = np.zeros(nthresh)\n",
    "fpr = np.zeros(nthresh)\n",
    "toter = np.zeros(nthresh)\n",
    "reduc = np.zeros(nthresh)\n",
    "y_true = y_test[:,1]\n",
    "\n",
    "for i in range(nthresh):\n",
    "    \n",
    "    y_pred = np.multiply((y_pred_class[:,1] > thresher[i]),1)\n",
    "    \n",
    "    tp = sum(y_pred*y_true)\n",
    "    tn = sum((1-y_pred)*(1-y_true))\n",
    "    fn = sum((1-y_pred)*y_true)\n",
    "    fp = sum(y_pred*(1-y_true))\n",
    "\n",
    "    toter[i] = (fn+fp)/(len(y_true))\n",
    "    fnr[i] = fn / [fn + tp]\n",
    "    fpr[i] = fp / [fp + tn]\n",
    "    reduc[i] = (tn + fn)/len(y_true)\n",
    "    \n",
    "sumerate = fpr + fnr\n",
    "minsumthresh = np.argmin(sumerate)\n",
    "minthresh = np.argmin(toter)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(thresher,fnr)\n",
    "plt.plot(thresher,fpr)\n",
    "plt.plot(thresher,fpr+fnr)\n",
    "plt.plot(thresher,toter)\n",
    "plt.legend(('FNR','FPR','Sum','Error Rate'))\n",
    "plt.grid()\n",
    "plt.ylim((0,.5))\n",
    "plt.scatter(thresher[minsumthresh],sumerate[minsumthresh])\n",
    "plt.scatter(thresher[minthresh],sumerate[minthresh])\n",
    "\n",
    "optreduc = (sum(1-y_true))/len(y_true)\n",
    "\n",
    "print('Best Sum Error is ' + str(round(sumerate[minsumthresh]*100,2)) + '% At threshold of ' + str(round(thresher[minsumthresh],2)))\n",
    "print('Best Error is ' + str(round(toter[minthresh]*100,2)) + '% At threshold of ' + str(round(thresher[minthresh],2)))\n",
    "print('Corresponds to FNR of ' + str(round(100*fnr[minthresh],2)) + '%')\n",
    "print('Corresponds to FPR of ' + str(round(100*fpr[minthresh],2)) + '%')\n",
    "print('Reduces by ' + str(round(100*reduc[minthresh],2)) + '%')\n",
    "print('Optimal reduction is' + str(round(100*optreduc)) + '%')\n",
    "print('Efficiency is ' + str(round(100*reduc[minthresh]/optreduc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmocean \n",
    "\n",
    "Ebins = np.linspace(-5,3,51)\n",
    "Hbins = np.linspace(0,4,41)\n",
    "Cbins = np.linspace(0,1,31)\n",
    "\n",
    "testE = np.sum(X_test[:,:25]*dFreq.T,axis=1)\n",
    "testH = X_test[:,-2]\n",
    "testC = X_test[:,-1]\n",
    "\n",
    "Eloc = np.digitize(np.log10(testE),Ebins,right=True)\n",
    "Hloc = np.digitize(testH,Hbins,right=True)\n",
    "Cloc = np.digitize(testC,Cbins,right=True)\n",
    "\n",
    "y_pred = np.multiply((y_pred_class[:,1] > thresher[minthresh]),1)\n",
    "errmat = np.abs((y_true - y_pred))\n",
    "\n",
    "\n",
    "accummat_error_EH = np.zeros((len(Ebins),len(Hbins)))\n",
    "accummat_num_EH = np.zeros((len(Ebins),len(Hbins)))\n",
    "accummat_numgo_EH = np.zeros((len(Ebins),len(Hbins)))\n",
    "accummat_numno_EH = np.zeros((len(Ebins),len(Hbins)))\n",
    "\n",
    "\n",
    "accummat_error_EC = np.zeros((len(Ebins),len(Cbins)))\n",
    "accummat_num_EC = np.zeros((len(Ebins),len(Cbins)))\n",
    "accummat_numgo_EC = np.zeros((len(Ebins),len(Cbins)))\n",
    "accummat_numno_EC = np.zeros((len(Ebins),len(Cbins)))\n",
    "\n",
    "accummat_error_CH = np.zeros((len(Cbins),len(Hbins)))\n",
    "accummat_num_CH = np.zeros((len(Cbins),len(Hbins)))\n",
    "accummat_numgo_CH = np.zeros((len(Cbins),len(Hbins)))\n",
    "accummat_numno_CH = np.zeros((len(Cbins),len(Hbins)))\n",
    "\n",
    "for i in range(len(Ebins)):\n",
    "    for j in range(len(Hbins)):\n",
    "        accummat_error_EH[i,j] = np.nanmean(errmat[(Eloc==i)& (Hloc==j)])\n",
    "        accummat_num_EH[i,j] = len(errmat[(Eloc==i)& (Hloc==j)])\n",
    "        accummat_numgo_EH[i,j] = np.sum(y_true[(Eloc==i)& (Hloc==j)])\n",
    "        accummat_numno_EH[i,j] = np.sum(y_pred[(Eloc==i)& (Hloc==j)])\n",
    "  \n",
    "for i in range(len(Ebins)):\n",
    "    for j in range(len(Cbins)):\n",
    "        accummat_error_EC[i,j] = np.nanmean(errmat[(Eloc==i)& (Cloc==j)])\n",
    "        accummat_num_EC[i,j] = len(errmat[(Eloc==i)& (Cloc==j)])\n",
    "        accummat_numgo_EC[i,j] = np.sum(y_true[(Eloc==i)& (Cloc==j)])\n",
    "        accummat_numno_EC[i,j] = np.sum(y_pred[(Eloc==i)& (Cloc==j)])\n",
    "\n",
    "\n",
    "for i in range(len(Cbins)):\n",
    "    for j in range(len(Hbins)):\n",
    "        accummat_error_CH[i,j] = np.nanmean(errmat[(Cloc==i)& (Hloc==j)])\n",
    "        accummat_num_CH[i,j] = len(errmat[(Cloc==i)& (Hloc==j)])\n",
    "        accummat_numgo_CH[i,j] = np.sum(y_true[(Cloc==i)& (Hloc==j)])\n",
    "        accummat_numno_CH[i,j] = np.sum(y_pred[(Cloc==i)& (Hloc==j)])\n",
    "\n",
    "\n",
    "accummat_numgo_EH = accummat_numgo_EH / accummat_num_EH\n",
    "accummat_numgo_EC = accummat_numgo_EC / accummat_num_EC\n",
    "accummat_numgo_CH = accummat_numgo_CH / accummat_num_CH\n",
    "\n",
    "accummat_numno_EH = accummat_numno_EH / accummat_num_EH\n",
    "accummat_numno_EC = accummat_numno_EC / accummat_num_EC\n",
    "accummat_numno_CH = accummat_numno_CH / accummat_num_CH\n",
    "\n",
    "\n",
    "accummat_num_EH = accummat_num_EH / np.nansum(accummat_num_EH) \n",
    "accummat_num_EC = accummat_num_EC / np.nansum(accummat_num_EC)\n",
    "accummat_num_CH = accummat_num_CH / np.nansum(accummat_num_CH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(14, 10), dpi=80, facecolor='w', edgecolor='k');\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.pcolormesh(Hbins,Ebins,accummat_error_EH,cmap='cmo.balance');\n",
    "plt.colorbar()\n",
    "plt.title('Avg. SAE');\n",
    "plt.xlabel('Ice Thickness')\n",
    "plt.ylabel('log Energy')\n",
    "# plt.clim([0,2])\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.pcolormesh(Cbins,Ebins,accummat_error_EC,cmap='cmo.balance');\n",
    "plt.colorbar()\n",
    "plt.title('Avg. SAE');\n",
    "plt.xlabel('Ice Conc')\n",
    "plt.ylabel('log Energy')\n",
    "# plt.clim([0,2])\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.pcolormesh(Hbins,Cbins,accummat_error_CH,cmap='cmo.balance');\n",
    "plt.colorbar()\n",
    "plt.title('Avg. SAE');\n",
    "plt.xlabel('Ice Thickness')\n",
    "plt.ylabel('Ice Conc')\n",
    "# plt.clim([0,2])\n",
    "\n",
    "plt.subplot(234)\n",
    "plotter = np.nanmean(accummat_error_EH,axis=1);\n",
    "plt.plot(Ebins,plotter)\n",
    "plotter = np.nansum(accummat_num_EH,axis=1)\n",
    "plt.plot(Ebins,plotter)\n",
    "plotter = np.nanmean(accummat_numgo_EH,axis=1)\n",
    "plt.plot(Ebins,plotter)\n",
    "plotter = np.nanmean(accummat_numno_EH,axis=1)\n",
    "plt.plot(Ebins,plotter)\n",
    "plt.title('SAE')\n",
    "plt.xlabel('Log Energy (m)')\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([-3,2])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(235)\n",
    "plotter = np.nanmean(accummat_error_EH[:,:],axis=0);\n",
    "plt.plot(Hbins,plotter)\n",
    "plotter = np.nansum(accummat_num_EH[:,:],axis=0)\n",
    "plt.plot(Hbins,plotter)\n",
    "plotter = np.nanmean(accummat_numgo_EH[:,:],axis=0)\n",
    "plt.plot(Hbins,plotter)\n",
    "plotter = np.nanmean(accummat_numno_EH[:,:],axis=0)\n",
    "plt.plot(Hbins,plotter)\n",
    "plt.title('SAE')\n",
    "plt.xlabel('Ice Thickness (m)')\n",
    "plt.ylim([0,1])\n",
    "plt.legend(('Error','Number'))\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(236)\n",
    "plotter = np.nanmean(accummat_error_CH[:,:],axis=1);\n",
    "plt.plot(Cbins,plotter)\n",
    "plotter = np.nansum(accummat_num_CH[:,:],axis=1)\n",
    "plt.plot(Cbins,plotter)\n",
    "plotter = np.nanmean(accummat_numgo_CH[:,:],axis=1)\n",
    "plt.plot(Cbins,plotter)\n",
    "plotter = np.nanmean(accummat_numno_CH[:,:],axis=1)\n",
    "\n",
    "plt.plot(Cbins,plotter)\n",
    "plt.title('SAE')\n",
    "plt.xlabel('Ice Conc (m)')\n",
    "plt.ylim([0,1])\n",
    "plt.legend(('Error','Relative Number','Frac Running','Frac Running (pred)'))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "#fig.tight_layout()\n",
    "plt.savefig('Fullnet-Error-withlast.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "thresh_opt = thresher[minthresh]\n",
    "y_pred = np.multiply((y_pred_class[:,1] > thresh_opt),1)\n",
    "\n",
    "\n",
    "index = random.choices(np.arange(len(X_test)), k=1000) # Any type\n",
    "\n",
    "WE = np.log10(np.sum(X_test[index,:25]*np.expand_dims(dFreq,axis=1).T,axis=2).T)\n",
    "H = np.expand_dims(X_test[index,-2],axis=1)\n",
    "C = np.expand_dims(X_test[index,-1],axis=1)\n",
    "USE = np.expand_dims(y_pred[index],axis=1) # Whether the input leads to fracture\n",
    "\n",
    "df = pd.DataFrame(np.concatenate((WE,H,USE),axis=1),columns=['Wave Energy','Ice Thickness','Usable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(\n",
    "    x='Ice Thickness',\n",
    "    y='Wave Energy',\n",
    "    data=df,    \n",
    "    hue='Usable',\n",
    ")\n",
    "# both, hue and size are optional\n",
    "sns.despine() # prettier layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat,savemat\n",
    "save_string = '/Users/chorvat/Dropbox (Brown)/Research Projects/Active/Data/Neural_Net_Data/6-hourly-2009/Classifier-results.mat'\n",
    "mdic = {\"Y_true\": y_test[:,1], \"Y_pred\": y_pred_class[:,1], \"X_test\":X_test}\n",
    "savemat(save_string,mdic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
